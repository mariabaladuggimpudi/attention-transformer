In Natural Language Processing (NLP), the preprocessing steps involve:

Tokenization
Stemming
Lemmatization
By applying these techniques, we aim to normalize the text. Usually, tokenization is the first step, and it comes in various forms depending on the NLP task. Stemming and lemmatization are employed after tokenization. Different types of tokenization techniques include:

Word Tokenization
Sentence Tokenization: Primarily used in tasks like text summarization.
Subword Tokenization: Divides words into smaller linguistic units.
Character Tokenization
Byte-Pair Encoding
Whitespace Tokenization
BERT Tokenization
SentencePiece Tokenization
WordPiece Tokenization
Noteworthy tokenizers include WordPiece, SentencePiece, and Byte-Pair Encoding. You can find these tokenizers in the transformers library.

Subword Tokenization is crucial for capturing context and constraining vocabulary. It stands apart from white space or word tokenization.

Byte-Pair Encoding (BPE) starts with a vocabulary comprising all characters. It iteratively pairs characters and updates the vocabulary based on their occurrence in the corpus. This process continues for a specified number of iterations or until a stopping criterion is met.

WordPiece is akin to BPE but with the distinction that it can merge entire tokens if they appear in the corpus.